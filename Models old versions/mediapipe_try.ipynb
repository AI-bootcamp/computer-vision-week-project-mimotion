{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88bc1027",
   "metadata": {},
   "source": [
    "Starter code to try the Mediapipe\n",
    "face mesh model: adds a complete mapping of the face. The model outputs an estimate of 478 3-dimensional face landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a95485",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m cap = cv2.VideoCapture(\u001b[32m0\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m cap.isOpened():\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     success, frame = \u001b[43mcap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[32m     14\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh()\n",
    "\n",
    "# for the camera capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = face_mesh.process(frame_rgb)\n",
    "\n",
    "# draw landmarks if any face is detected\n",
    "    if result.multi_face_landmarks:\n",
    "        for face_landmarks in result.multi_face_landmarks:\n",
    "            mp.solutions.drawing_utils.draw_landmarks(\n",
    "                frame, face_landmarks, mp_face_mesh.FACEMESH_TESSELATION)\n",
    "\n",
    "    # video stream\n",
    "    cv2.imshow('Mimotion Face Mesh', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab957bb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# ÿ™ÿ≠ŸàŸäŸÑ ÿßŸÑÿ£ŸÑŸàÿßŸÜ\u001b[39;00m\n\u001b[32m     24\u001b[39m rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m results = \u001b[43mface_mesh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# ÿ±ÿ≥ŸÖ ÿßŸÑŸÜŸÇÿßÿ∑\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m results.multi_face_landmarks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mediapipe\\python\\solutions\\face_mesh.py:125\u001b[39m, in \u001b[36mFaceMesh.process\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np.ndarray) -> NamedTuple:\n\u001b[32m    111\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Processes an RGB image and returns the face landmarks on each detected face.\u001b[39;00m\n\u001b[32m    112\u001b[39m \n\u001b[32m    113\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m \u001b[33;03m    face landmarks on each detected face.\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[39m, in \u001b[36mSolutionBase.process\u001b[39m\u001b[34m(self, input_data)\u001b[39m\n\u001b[32m    334\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    335\u001b[39m     \u001b[38;5;28mself\u001b[39m._graph.add_packet_to_input_stream(\n\u001b[32m    336\u001b[39m         stream=stream_name,\n\u001b[32m    337\u001b[39m         packet=\u001b[38;5;28mself\u001b[39m._make_packet(input_stream_type,\n\u001b[32m    338\u001b[39m                                  data).at(\u001b[38;5;28mself\u001b[39m._simulated_timestamp))\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# ŸÉÿßŸÖŸäÿ±ÿß\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "LEFT_EYE = [33, 133]\n",
    "RIGHT_EYE = [362, 263]\n",
    "LEFT_EYEBROW = [55, 65]\n",
    "RIGHT_EYEBROW = [285, 295]\n",
    "MOUTH = [61, 291]\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for landmarks in results.multi_face_landmarks:\n",
    "            h, w, _ = frame.shape\n",
    "            # ÿπŸäŸÜ Ÿäÿ≥ÿßÿ±\n",
    "            for idx in LEFT_EYE:\n",
    "                x, y = int(landmarks.landmark[idx].x * w), int(landmarks.landmark[idx].y * h)\n",
    "                cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "            # ÿπŸäŸÜ ŸäŸÖŸäŸÜ\n",
    "            for idx in RIGHT_EYE:\n",
    "                x, y = int(landmarks.landmark[idx].x * w), int(landmarks.landmark[idx].y * h)\n",
    "                cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "            # ÿßŸÑÿ≠ÿßÿ¨ÿ® Ÿäÿ≥ÿßÿ±\n",
    "            for idx in LEFT_EYEBROW:\n",
    "                x, y = int(landmarks.landmark[idx].x * w), int(landmarks.landmark[idx].y * h)\n",
    "                cv2.circle(frame, (x, y), 3, (255, 0, 0), -1)\n",
    "            # ÿßŸÑÿ≠ÿßÿ¨ÿ® ŸäŸÖŸäŸÜ\n",
    "            for idx in RIGHT_EYEBROW:\n",
    "                x, y = int(landmarks.landmark[idx].x * w), int(landmarks.landmark[idx].y * h)\n",
    "                cv2.circle(frame, (x, y), 3, (255, 0, 0), -1)\n",
    "            # ÿßŸÑŸÅŸÖ\n",
    "            for idx in MOUTH:\n",
    "                x, y = int(landmarks.landmark[idx].x * w), int(landmarks.landmark[idx].y * h)\n",
    "                cv2.circle(frame, (x, y), 3, (0, 0, 255), -1)\n",
    "\n",
    "    cv2.imshow(\"Selected Points\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54dfe833",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m frame = cv2.flip(frame, \u001b[32m1\u001b[39m)\n\u001b[32m     42\u001b[39m img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m results = \u001b[43mface_mesh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m results.multi_face_landmarks:\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m face_landmarks \u001b[38;5;129;01min\u001b[39;00m results.multi_face_landmarks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mediapipe\\python\\solutions\\face_mesh.py:125\u001b[39m, in \u001b[36mFaceMesh.process\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np.ndarray) -> NamedTuple:\n\u001b[32m    111\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Processes an RGB image and returns the face landmarks on each detected face.\u001b[39;00m\n\u001b[32m    112\u001b[39m \n\u001b[32m    113\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m \u001b[33;03m    face landmarks on each detected face.\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[39m, in \u001b[36mSolutionBase.process\u001b[39m\u001b[34m(self, input_data)\u001b[39m\n\u001b[32m    334\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    335\u001b[39m     \u001b[38;5;28mself\u001b[39m._graph.add_packet_to_input_stream(\n\u001b[32m    336\u001b[39m         stream=stream_name,\n\u001b[32m    337\u001b[39m         packet=\u001b[38;5;28mself\u001b[39m._make_packet(input_stream_type,\n\u001b[32m    338\u001b[39m                                  data).at(\u001b[38;5;28mself\u001b[39m._simulated_timestamp))\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# start with MediaPipe\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# the points of face landmarks\n",
    "LEFT_EYE = [33, 133]\n",
    "RIGHT_EYE = [362, 263]\n",
    "MOUTH = [61, 291]\n",
    "LEFT_EYEBROW = [55, 65]\n",
    "RIGHT_EYEBROW = [285, 295]\n",
    "\n",
    "def calc_distance(p1, p2):\n",
    "    return np.linalg.norm(np.array(p1) - np.array(p2))\n",
    "\n",
    "def predict_emotion(left_eye_open, right_eye_open, mouth_open, left_eyebrow_lift, right_eyebrow_lift):\n",
    "    if mouth_open > 25 and left_eye_open > 10 and right_eye_open > 10:\n",
    "        return \"Surprised üò≤\"\n",
    "    elif mouth_open > 20 and left_eye_open > 5 and right_eye_open > 5:\n",
    "        return \"Happy üòÅ\"\n",
    "    elif left_eyebrow_lift < 10 and right_eyebrow_lift < 10:\n",
    "        return \"Sad üò¢\"\n",
    "    elif left_eyebrow_lift > 20 and right_eyebrow_lift > 20:\n",
    "        return \"Angry üò†\"\n",
    "    else:\n",
    "        return \"Neutral üòê\"\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(img_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            h, w, _ = frame.shape\n",
    "\n",
    "            def get_landmark(idx):\n",
    "                return (int(face_landmarks.landmark[idx].x * w), int(face_landmarks.landmark[idx].y * h))\n",
    "\n",
    "            left_eye = [get_landmark(LEFT_EYE[0]), get_landmark(LEFT_EYE[1])]\n",
    "            right_eye = [get_landmark(RIGHT_EYE[0]), get_landmark(RIGHT_EYE[1])]\n",
    "            mouth = [get_landmark(MOUTH[0]), get_landmark(MOUTH[1])]\n",
    "            left_eyebrow = [get_landmark(LEFT_EYEBROW[0]), get_landmark(LEFT_EYEBROW[1])]\n",
    "            right_eyebrow = [get_landmark(RIGHT_EYEBROW[0]), get_landmark(RIGHT_EYEBROW[1])]\n",
    "\n",
    "            # calculate distances\n",
    "            left_eye_open = calc_distance(left_eye[0], left_eye[1])\n",
    "            right_eye_open = calc_distance(right_eye[0], right_eye[1])\n",
    "            mouth_open = calc_distance(mouth[0], mouth[1])\n",
    "            left_eyebrow_lift = calc_distance(left_eyebrow[0], left_eyebrow[1])\n",
    "            right_eyebrow_lift = calc_distance(right_eyebrow[0], right_eyebrow[1])\n",
    "\n",
    "            \n",
    "            emotion = predict_emotion(left_eye_open, right_eye_open, mouth_open, left_eyebrow_lift, right_eyebrow_lift)\n",
    "\n",
    "            \n",
    "            cv2.putText(frame, f'Emotion: {emotion}', (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow('Mimotion - Emotion Detection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b727901e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m frame = cv2.flip(frame, \u001b[32m1\u001b[39m)\n\u001b[32m     39\u001b[39m rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m results = \u001b[43mface_mesh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m results.multi_face_landmarks:\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m landmarks \u001b[38;5;129;01min\u001b[39;00m results.multi_face_landmarks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mediapipe\\python\\solutions\\face_mesh.py:125\u001b[39m, in \u001b[36mFaceMesh.process\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np.ndarray) -> NamedTuple:\n\u001b[32m    111\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Processes an RGB image and returns the face landmarks on each detected face.\u001b[39;00m\n\u001b[32m    112\u001b[39m \n\u001b[32m    113\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m \u001b[33;03m    face landmarks on each detected face.\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[39m, in \u001b[36mSolutionBase.process\u001b[39m\u001b[34m(self, input_data)\u001b[39m\n\u001b[32m    334\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    335\u001b[39m     \u001b[38;5;28mself\u001b[39m._graph.add_packet_to_input_stream(\n\u001b[32m    336\u001b[39m         stream=stream_name,\n\u001b[32m    337\u001b[39m         packet=\u001b[38;5;28mself\u001b[39m._make_packet(input_stream_type,\n\u001b[32m    338\u001b[39m                                  data).at(\u001b[38;5;28mself\u001b[39m._simulated_timestamp))\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# MediaPipe\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True)\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "LEFT_EYE_INDICES = [33, 160, 158, 133, 153, 144]\n",
    "RIGHT_EYE_INDICES = [362, 385, 387, 263, 373, 380]\n",
    "MOUTH_INDICES = [78, 308, 13, 14]\n",
    "\n",
    "def calc_eye_aspect_ratio(eye_points):\n",
    "    # ÿ≠ÿ≥ÿßÿ® ÿßŸÑŸÜÿ≥ÿ®ÿ© ÿ®ŸäŸÜ ÿßŸÑÿ∑ŸàŸÑ ŸàÿßŸÑÿπÿ±ÿ∂ ŸÑŸÑÿπŸäŸÜ\n",
    "    p1, p2, p3, p4, p5, p6 = eye_points\n",
    "    vertical1 = np.linalg.norm(np.array(p2) - np.array(p6))\n",
    "    vertical2 = np.linalg.norm(np.array(p3) - np.array(p5))\n",
    "    horizontal = np.linalg.norm(np.array(p1) - np.array(p4))\n",
    "    ear = (vertical1 + vertical2) / (2.0 * horizontal)\n",
    "    return ear\n",
    "\n",
    "def calc_mouth_open(mouth_points):\n",
    "    # ÿ≠ÿ≥ÿßÿ® ŸÅÿ™ÿ≠ ÿßŸÑŸÅŸÖ\n",
    "    top = np.array(mouth_points[2])\n",
    "    bottom = np.array(mouth_points[3])\n",
    "    distance = np.linalg.norm(top - bottom)\n",
    "    return distance\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for landmarks in results.multi_face_landmarks:\n",
    "            h, w, _ = frame.shape\n",
    "\n",
    "            def get_point(idx):\n",
    "                return (int(landmarks.landmark[idx].x * w), int(landmarks.landmark[idx].y * h))\n",
    "\n",
    "            left_eye = [get_point(i) for i in LEFT_EYE_INDICES]\n",
    "            right_eye = [get_point(i) for i in RIGHT_EYE_INDICES]\n",
    "            mouth = [get_point(i) for i in MOUTH_INDICES]\n",
    "\n",
    "            left_ear = calc_eye_aspect_ratio(left_eye)\n",
    "            right_ear = calc_eye_aspect_ratio(right_eye)\n",
    "            mouth_open = calc_mouth_open(mouth)\n",
    "\n",
    "            avg_ear = (left_ear + right_ear) / 2.0\n",
    "\n",
    "            # ÿ™ÿ≠ÿØŸäÿØ ÿßŸÑŸÖÿ¥ÿßÿπÿ± \n",
    "            emotion = \"Neutral üòê\"\n",
    "            if mouth_open > 30 and avg_ear > 0.25:\n",
    "                emotion = \"Surprised üò≤\"\n",
    "            elif mouth_open > 20:\n",
    "                emotion = \"Happy üòÅ\"\n",
    "            elif avg_ear < 0.2:\n",
    "                emotion = \"Sad üò¢\"\n",
    "            else:\n",
    "                emotion = \"Neutral üòê\"\n",
    "\n",
    "            cv2.putText(frame, f'Emotion: {emotion}', (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Improved Mimotion - Emotion Detection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
